{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://corise:corise@localhost:5432/dbt\n",
    "%config SqlMagic.displaylimit=10\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling challenge\n",
    "Let’s say that the Director of Product at greenery comes to us (the head Analytics Engineer) and asks some questions:\n",
    "\n",
    "How are our users moving through the product funnel?\n",
    "\n",
    "Which steps in the funnel have largest drop off points?\n",
    "\n",
    "Product funnel is defined with 3 levels for our dataset:\n",
    "- Sessions with any event of type page_view / add_to_cart / checkout\n",
    "- Sessions with any event of type add_to_cart / checkout\n",
    "- Sessions with any event of type checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(psycopg2.errors.UndefinedTable) relation \"staging.stg_events\" does not exist\n",
      "LINE 5:   from staging.stg_events\n",
      "               ^\n",
      "\n",
      "[SQL: with visitors as (\n",
      "  select\n",
      "    session_id, -- effectively a user_id\n",
      "    min(created_at) as min_time -- gets the earliest Visit for each person\n",
      "  from staging.stg_events\n",
      "  group by 1\n",
      "),\n",
      "\n",
      "page_views as (\n",
      "  select\n",
      "    distinct e.session_id\n",
      "  from visitors v -- ensures we only look at the Visitors defined above\n",
      "  inner join staging.stg_events e on e.session_id = v.session_id\n",
      "  where e.event_type= 'page_view' -- an internal event that defines sign-up\n",
      "),\n",
      "\n",
      "add_to_cart as (\n",
      "  select\n",
      "    distinct e.session_id\n",
      "  from page_views s  -- ensures we only look at the Signups defined above\n",
      "  inner join staging.stg_events e on e.session_id= s.session_id\n",
      "  where e.event_type= 'add_to_cart'\n",
      "),\n",
      "\n",
      "checkouts as (\n",
      "  select\n",
      "    distinct e.session_id\n",
      "  from add_to_cart  a -- ensures we only look at the Activations defined above\n",
      "  inner join staging.stg_events e on e.session_id = a.session_id\n",
      "  where e.event_type = 'checkout'   \n",
      "),\n",
      "\n",
      "\n",
      "steps as (\n",
      "  select 'Visits' as step, COUNT(*) from visitors\n",
      "    union\n",
      "   select 'Page Views' as step, COUNT(*) from page_views\n",
      "    union\n",
      "  select 'Add to Carts' as step, COUNT(*) from add_to_cart\n",
      "    union\n",
      "  select 'Checkouts' as step, COUNT(*) from checkouts\n",
      "  order by count desc\n",
      ")\n",
      "\n",
      "select\n",
      "  step,\n",
      "  count,\n",
      "  lag(count, 1) over (),\n",
      "  round((1.0 - count::numeric/lag(count, 1) over ()),2) as drop_off\n",
      "\n",
      "from steps]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "with visitors as (\n",
    "  select\n",
    "    session_id, -- effectively a user_id\n",
    "    min(created_at) as min_time -- gets the earliest Visit for each person\n",
    "  from staging.stg_events\n",
    "  group by 1\n",
    "),\n",
    "\n",
    "page_views as (\n",
    "  select\n",
    "    distinct e.session_id\n",
    "  from visitors v -- ensures we only look at the Visitors defined above\n",
    "  inner join staging.stg_events e on e.session_id = v.session_id\n",
    "  where e.event_type= 'page_view' -- an internal event that defines sign-up\n",
    "),\n",
    "\n",
    "add_to_cart as (\n",
    "  select\n",
    "    distinct e.session_id\n",
    "  from page_views s  -- ensures we only look at the Signups defined above\n",
    "  inner join staging.stg_events e on e.session_id= s.session_id\n",
    "  where e.event_type= 'add_to_cart'\n",
    "),\n",
    "\n",
    "checkouts as (\n",
    "  select\n",
    "    distinct e.session_id\n",
    "  from add_to_cart  a -- ensures we only look at the Activations defined above\n",
    "  inner join staging.stg_events e on e.session_id = a.session_id\n",
    "  where e.event_type = 'checkout'   \n",
    "),\n",
    "\n",
    "\n",
    "steps as (\n",
    "  select 'Visits' as step, COUNT(*) from visitors\n",
    "    union\n",
    "   select 'Page Views' as step, COUNT(*) from page_views\n",
    "    union\n",
    "  select 'Add to Carts' as step, COUNT(*) from add_to_cart\n",
    "    union\n",
    "  select 'Checkouts' as step, COUNT(*) from checkouts\n",
    "  order by count desc\n",
    ")\n",
    "\n",
    "select\n",
    "  step,\n",
    "  count,\n",
    "  lag(count, 1) over (),\n",
    "  round((1.0 - count::numeric/lag(count, 1) over ()),2) as drop_off\n",
    "\n",
    "from steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If your organization is thinking about using dbt, how would you pitch the value of dbt/analytics engineering to a decision maker at your organization?\n",
    "- I would pitch the ability to better version control data modeling logic, and present it in visual DAGs so other analysts can conceptualize where data is coming from.\n",
    "\n",
    "#### If you are thinking about moving to analytics engineering, what skills have you picked that give you the most confidence in pursuing this next step?\n",
    "- Learning new ways to model events/web data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for production / scheduled dbt run of your project And finally, before you fly free into the dbt night, we will take a step back and reflect: after learning about the various options for dbt deployment and seeing your final dbt project, how would you go about setting up a production/scheduled dbt run of your project in an ideal state? You don’t have to actually set anything up - just jot down what you would do and why and post in a README file.\n",
    "\n",
    "Hints: what steps would you have? Which orchestration tool(s) would you be interested in using? What schedule would you run your project on? Which metadata would you be interested in using? How/why would you use the specific metadata? , etc.\n",
    "\n",
    "- I would look into dagster, prefect, airflow to set up the orchestation of dbt. Dagster looks to have an impressive integration with them, however only self-hosted options seem to exist currently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 DAG:\n",
    "\n",
    "![Week4Dag](images/dbt-dag-week4.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625c31d6b4db3d7e7e2853cc30dc2062e1cda684f3e49d5f899ae496ae755fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
